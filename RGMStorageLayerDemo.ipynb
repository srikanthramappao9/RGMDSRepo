{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2070d59b-b6b3-4c02-80fb-25101ea9301b",
   "metadata": {
    "editable": false
   },
   "outputs": [],
   "source": [
    "# This cell is NOT editable. Overwrite variables on your own discretion.\n",
    "# Any changes other than the script code will NOT BE SAVED!\n",
    "# All cells are assumed to be script code cells, unless explictly tagged as 'o9_ignore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c58d578-de82-415f-bedb-1442650a731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_inp_data1 = \"Select (&CWV * [Initiative].[Initiative] * [Location].[Location]*[Location].[Location Country] * [Sales Domain].[Ship To] * [Sales Domain].[Channel] * [Sales Domain].[Country] * [Sales Domain].[Region] * [Time].[Week] * [Item].[PPG] ) on row,    ({Measure.[RGM Promo Simulation Initiative Planned Base PTC],Measure.[RGM Promo Simulation Initiative Planned Promo PTC], Measure.[RGM Initiative Week Planned Promo Discount], Measure.[RGM Initiative Week Planned %ACV],Measure.[RGM Initiative Week Planned Promo %ACV], Measure.[RGM Initiative Week Planned Effective Discount],Measure.[RGM Initiative Week Planned Display Type], Measure.[RGM Initiative Week Planned Promo Mechanic]}) on column  where {~isnull (Measure.[RGM Promo Simulation Initiative Planned Base PTC])};\"\n",
    "\n",
    "\n",
    "# Initialize the O9DataLake with the input parameters and dataframes\n",
    "# Data can be accessed with O9DataLake.get(<Input Name>)\n",
    "# Overwritten values will not be reflected in the O9DataLake after initialization\n",
    "\n",
    "from o9_common_utils.O9DataLake import O9DataLake, ResourceType, DataSource\n",
    "O9DataLake.register(\"inp_data1\",DataSource.LS, ResourceType.IBPL, _inp_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d164d9b7-8939-4cfa-a05a-afff0808eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "#\n",
    "from o9helpers import user_storage_path  ##########Necessary argument to import storage path\n",
    "from o9storage import cloud_storage_utils ##########Necessary argument to import cloud storage path\n",
    "import os   ##########Argument to install os\n",
    "import logging ##########Necessary argument to import logging package\n",
    "import shutil  ##########Argument to operate with file objects\n",
    "from os import path \n",
    "\n",
    "logger = logging.getLogger('o9_logger')\n",
    "logger.info('logger initialized')\n",
    "\n",
    "#inputs\n",
    "logger.info('reading input')\n",
    "RawSellout = inp_data1\n",
    "logger.info(RawSellout.columns)\n",
    "\n",
    "RawSellout['Version.[Version Name]']='sr_test'\n",
    "\n",
    "#Output1=RawSellout \n",
    "logger.info('demo storagelayer code ended')\n",
    "\n",
    "#\"\"\"\n",
    "#bucket contains all files,bucket name can be user specified.bucket name should be changed from plug-in to plug-in.\n",
    "#when you are having slice other than version(1 slice),bucket should be specified with \"bucket_{}\".format(df_keys['SliceKey.[Dept]']) format\n",
    "#so that when saving files in storage it get's saved with slice name .\n",
    "#\"\"\"\n",
    "bucket = \"bucket_15\"   \n",
    "local_storage_path = os.path.join(user_storage_path, bucket) \n",
    "\n",
    "######Each user will have a user storage path,joining user storage path with bucket.\n",
    "test_folder_path = os.path.join(local_storage_path, \"test\")\n",
    "\n",
    "#######creating a folder in loacal storage path,In this example test will be folder name in local storage path.\n",
    "os.makedirs(test_folder_path)\n",
    "\n",
    "######making test folder path as directory.\n",
    "csv_path = os.path.join(test_folder_path, 'RawSellout.csv')\n",
    "\n",
    "logger.info('#########################demo storagelayer csv_path##################')\n",
    "logger.info(csv_path)\n",
    "#######path to save files.\n",
    "RawSellout.to_csv(csv_path,index=False) \n",
    "logger.info(RawSellout)\n",
    "\n",
    "\n",
    "#\"\"\"\n",
    "#Now all the files have been saved in the bucket.\n",
    "##Now we have to push the bucket to cloud storage so that you can retrieve it wherever you want\n",
    "#\"\"\"\n",
    "try:\n",
    "    logger.debug('********Before storage_push*******')\n",
    "    logger.debug(os.listdir(local_storage_path))\n",
    "    value = cloud_storage_utils.storage_push(bucket, local_storage_path, overwrite=True) ##########syntax to push bucket to cloud storage\n",
    "    if value:\n",
    "        logger.debug(\"storage_push successful\")\n",
    "    else:\n",
    "        logger.debug(\"storage_push failed\")\n",
    "    logger.debug('********storage_push successful*******')\n",
    "    logger.debug(os.listdir(local_storage_path))\n",
    "    import glob\n",
    "    logger.info('********List of files in bucket after storage push************')\n",
    "    for filename in glob.iglob(local_storage_path + '**/**/*', recursive=True):\n",
    "        logger.debug(filename) #################this prints all files/path  present in storage. \n",
    "        logger.info('#########################aftell push  storagelayer ##################')\n",
    "\n",
    "except:\n",
    "    logger.exception(\"Couldn't commit folder\")\n",
    "\n",
    "#\"\"\"\n",
    "#Code for storage pull\n",
    "#\"\"\"\n",
    "shutil.rmtree(test_folder_path) #removing test folder path from bucket to verify storage pull\n",
    "logger.debug('******** 1.List of files before storage_pull*******')\n",
    "logger.debug(os.listdir(local_storage_path))\n",
    "for filename in glob.iglob(local_storage_path + '**/**/*', recursive=True):\n",
    "    logger.debug(filename)\n",
    "\n",
    "logger.info('2.Reading files from storage..............................................')\n",
    "local_storage_path = os.path.join(user_storage_path, bucket)\n",
    "value = cloud_storage_utils.storage_pull(bucket, local_storage_path, overwrite=True)\n",
    "import glob\n",
    "logger.debug('********3.List of files in bucket after storage pull************')\n",
    "for filename in glob.iglob(local_storage_path + '**/**/*', recursive=True):\n",
    "    logger.debug(filename)\n",
    "\n",
    "RawSellout=pd.read_csv(os.path.join(test_folder_path,'RawSellout.csv'))\n",
    "logger.debug('################## 4.RawSellout after pull #####################')\n",
    "logger.debug(RawSellout.columns)\n",
    "logger.debug(RawSellout.head())\n",
    "#output_df = None\n",
    "\n",
    "Output1=RawSellout\n",
    "logger.debug('********** 5.demo storage layer PULL Read the data from cloud storage **************')\n",
    "logger.debug(os.path.join(test_folder_path,'RawSellout.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbedf3c-2a29-43b5-82df-8949f7015025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[IRPPromotions] Tenant Conda Environment",
   "language": "python",
   "name": "sandbox_irppromotions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "notebook_dict": {
   "ClassName": "o9.GraphCube.Plugins.Python.PythonScript",
   "InstanceName": "RGMStorageLayerDemo",
   "SliceKeys": [],
   "file_path": "loaded_notebooks/RGMStorageLayerDemo.ipynb",
   "o9_selected_plugin_id": 193427
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
