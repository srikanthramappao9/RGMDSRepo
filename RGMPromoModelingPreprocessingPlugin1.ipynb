{"cells": [{"cell_type": "code", "id": "aa6ee01a-1560-4af9-92da-5997a37dd45a", "source": ["# This cell is NOT editable. Overwrite variables on your own discretion.\n# Any changes other than the script code will NOT BE SAVED!\n# All cells are assumed to be script code cells, unless explictly tagged as 'o9_ignore'"], "metadata": {"editable": false}, "tags": ["o9_ignore"]}, {"cell_type": "code", "id": "99b373c9-fcac-4cae-8b1a-e4cdf8aef617", "source": ["_sellOutData = \"Select (&CWVAndScenarios * [Region].[Region L3] * [Region].[Region L2] * &RGMPlanningRegions *  &RGMChannels * &RGMAccounts * [Time].[Week] * [Item].[RGM1] * [Item].[RGM2] * [Item].[RGM7] * [Item].[RGM5] * [Item].[RGM6] * [Item].[PPG]) on row, ({Measure.[RGM %ACV], Measure.[RGM Promo Simulation Sales], Measure.[RGM Value Sales], Measure.[RGM Promo Simulation Base PTC]}) on column;\"\n_tradePromoCalendarData = \"Select (&CWVAndScenarios * &PromotionInitiatives * [Region].[Region L3] * [Region].[Region L2] * &RGMPlanningRegions *  &RGMChannels * &RGMAccounts * &ClientPPGs * [Time].[Week]) on row, ({Measure.[RGM Initiative Week Sellout Start Date], Measure.[RGM Initiative Week Sellout End Date], Measure.[RGM Initiative Week Promo Mechanic], Measure.[RGM Promo Simulation Initiative Base PTC], Measure.[RGM Promo Simulation Initiative Promo PTC], Measure.[RGM Promo Simulation Initiative Base PTR], Measure.[RGM Promo Simulation Initiative Promo PTR], Measure.[RGM Initiative Week Promo Discount], Measure.[RGM Initiative Week Display Type]}) on column where {~isnull (Measure.[RGM Promo Simulation Initiative Base PTC])};\"\n_timeData = \"select ([Time].[Planning Year] * [Time].[Planning Month] * [Time].[Week] * [Time].[Week Name] * [Time].[Day]) on row, () on column include memberproperties { [Time].[Planning Year] , [Key] } { [Time].[Planning Month], [Key] } { [Time].[Week], [Key] } { [Time].[Day], [Key] };\"\n_inputConfigParameter1 = \"Select (&CWVAndScenarios) on row,  ({Measure.[RGM Historical Years], Measure.[RGM Seasonality Years], Measure.[RGM Learning Rate], Measure.[RGM ML Algorithms], Measure.[RGM Model Run Type],  Measure.[RGM Negative Correlation Threshold For Cannibalization], Measure.[RGM Percentage Change In Amplitude],  Measure.[RGM Positive Correlation Threshold For Halo], Measure.[RGM Seasonality Time Bucket]}) on column;\"\n_inputConfigParameter2 = \"Select (&CWVAndScenarios * [Region].[Region L3] * [Item].[RGM5] * Item.[RGM6]) on row,  ({Measure.[RGM Discount Pct Threshold], Measure.[RGM Pantry Decay Rate]}) on column;\"\n_auditDataFrame = \"Select (&CwvandScenarios * [Sequence].[RGM Series] * [Time].[Day] ) on row,  ({Measure.[RGM ML Algorithm History], Measure.[RGM Model Run Data End Date History], Measure.[RGM Model Run Data Start Date History], Measure.[RGM Model Run Timestamp History], Measure.[RGM Model Run Type History], Measure.[RGM Model Runtime History], Measure.[RGM Plugin Name History]}) on column;\"\n_tenantCurrDay = \"Select (&AllDays.Filter(#.[Day$IsCurrent]));\"\n_tenantSequenceID = \"Select (&CwvAndScenarios) on row,  ({Measure.[RGM Model Run Sequence ID1], Measure.[RGM Model Run Sequence ID2], Measure.[RGM Model Run Sequence ID3]}) on column;\"\n\n\n# Initialize the O9DataLake with the input parameters and dataframes\n# Data can be accessed with O9DataLake.get(<Input Name>)\n# Overwritten values will not be reflected in the O9DataLake after initialization\n\nfrom o9_common_utils.O9DataLake import O9DataLake, ResourceType, DataSource\nsellOutData = O9DataLake.register(\"sellOutData\",DataSource.LS, ResourceType.IBPL, _sellOutData)\ntradePromoCalendarData = O9DataLake.register(\"tradePromoCalendarData\",DataSource.LS, ResourceType.IBPL, _tradePromoCalendarData)\ntimeData = O9DataLake.register(\"timeData\",DataSource.LS, ResourceType.IBPL, _timeData)\ninputConfigParameter1 = O9DataLake.register(\"inputConfigParameter1\",DataSource.LS, ResourceType.IBPL, _inputConfigParameter1)\ninputConfigParameter2 = O9DataLake.register(\"inputConfigParameter2\",DataSource.LS, ResourceType.IBPL, _inputConfigParameter2)\nauditDataFrame = O9DataLake.register(\"auditDataFrame\",DataSource.LS, ResourceType.IBPL, _auditDataFrame)\ntenantCurrDay = O9DataLake.register(\"tenantCurrDay\",DataSource.LS, ResourceType.IBPL, _tenantCurrDay)\ntenantSequenceID = O9DataLake.register(\"tenantSequenceID\",DataSource.LS, ResourceType.IBPL, _tenantSequenceID)"], "metadata": {"tags": ["o9_ignore"]}, "tags": ["o9_ignore"]}, {"cell_type": "code", "id": "1582152a-166b-4f2c-9d18-d609aebc4ecd", "source": ["#!/usr/bin/env python\n# coding: utf-8\n\n# ## 2. RGM - Data Pre-processing (2. RGM Trade Promo Modelling - Preprocessing)\n\n# In[1]:\n\n\n# Import all the necessary libraries.\nimport os\nimport pandas as pd\nimport numpy as np\nimport datetime, time\nimport warnings\nwarnings.filterwarnings('ignore')\nlogger = logging.getLogger('o9_logger')\n\n\n# In[2]:\n\n\nstartTime = time.time()\nlogger.debug(\"Started running pre-processing code for promo model...\")\n\n\n# In[3]:\n\n\n#Enhancement log:\n#Config2.0:\n#1.UserStory-212123: Incorporate the Sales Domain Dimension Breakup / regraning dimensions name with new Name\n\n\n# In[4]:\n\n\n# Read all the input data.\n# sellOutData = pd.read_csv(\"Sellout DS Tenant.csv\")\n# tradePromoCalendarData = pd.read_csv(\"Promo Initiative Calendar DS Tenant.csv\")\n# #tradePromoCalendarData = pd.read_csv(\"Promo Initiative Calendar DS Tenant-empty.csv\") #UserStory:206082-(Config 2.0),Enhancing Pricing Model to work with & without Trade Promo Calendar Data\n# timeData = pd.read_csv(\"Time Master Tenant.csv\")\n# inputConfigParameter1 = pd.read_csv(\"Parameter 1 Tenant.csv\")\n# inputConfigParameter2 = pd.read_csv(\"Parameter 2 Tenant.csv\")\n# auditDataFrame = pd.read_csv('auditTableDataOutputFinal.csv')\n# tenantCurrDay = pd.read_csv(\"Dimension.AllDays.csv\")\n# tenantSequenceID = pd.read_csv(\"sequenceIDDataOutput_Plugin0.csv\")\n# # holidayData = pd.read_csv(\"Fact.Holiday.csv\")\n# # powerDateData = pd.read_csv(\"Fact.RGMPowerDates.csv\")\n# #\n# # weatherDataDailyLevel = pd.read_csv(\"Fact.WeatherData.csv\")\n# # googleMobilityDataWeeklyLevel = pd.read_csv(\"Fact.GoogleMobilityData.csv\")\n# # unemploymentDataMonthlyLevel = pd.read_csv(\"Fact.UnemploymentData.csv\")\n# # consumerSentimentDataMonthlyLevel = pd.read_csv(\"Fact.ConsumerSentimentData.csv\")\n# #\n# # loyaltyData = pd.read_csv(\"Fact.LoyaltyDataNew.csv\")\n\n\n# In[5]:\n\n\nlogger.debug(\"For sellOutData, shape is \" + str(sellOutData.shape))\nlogger.debug(\"For tradePromoCalendarData, shape is \" + str(tradePromoCalendarData.shape))\nlogger.debug(\"For timeData, shape is \" + str(timeData.shape))\nlogger.debug(\"For inputConfigParameter1, shape is \" + str(inputConfigParameter1.shape))\nlogger.debug(\"For inputConfigParameter2, shape is \" + str(inputConfigParameter2.shape))\nlogger.debug(\"For auditDataFrame, shape is \" + str(auditDataFrame.shape))\nlogger.debug(\"For tenantCurrDay, shape is \" + str(tenantCurrDay.shape))\nlogger.debug(\"For tenantSequenceID, shape is \" + str(tenantSequenceID.shape))\nlogger.debug(\"Successfully read the input data.\")\n\n\n# In[6]:\n\n\n# Variable Config For Dynamic Name\n\n# Sell Out Data Column Names\nversionVariable = 'Version.[Version Name]'\n# UserStory-212123\ncountryVariable='Region.[Region L3]' # Contains the Country info\nregionVariable = 'Region.[Region L2]' # Contains the Region info\nlocationVariable = 'Region.[Planning Region]' # Contains the store/store cluster/region data\nchannelVariable = 'Channel.[Channel]' # Contains the Channel info\ncustomerGroupVariable = 'Account.[Account]' # Name of customer group column/Contains the Account info\nlocationCountryVariable = 'Region.[Region L3]_duplicate' # (Region.[Region L3] will now contain the country info, to avoid the code change we are create the duplicate column )\n#\nrgm1Variable = 'Item.[RGM1]' # Self vs competitors column\nrgm2Variable = 'Item.[RGM2]' # Manufactur column\nrgm5Variable = 'Item.[RGM5]' # Category column\nrgm6Variable = 'Item.[RGM6]' # Sub-category column\nrgm7Variable = 'Item.[RGM7]' # Sub-Category + Brand column\nppgVariable = 'Item.[PPG]' # Name of PPG column\nunitSizeVariable = 'Item.[Unit Size]' # Unit size column\npackSizeVariable = 'Item.[Pack Size]' # Pack size column\nacvVariable = 'RGM %ACV' # Name of weighted distribution column\nvalueSalesVariable = 'RGM Value Sales' # Name of revenue column\ndependentVariable = 'RGM Promo Simulation Sales' # Name of dependent variable for the modelling\nbasePriceVariable = 'RGM Promo Simulation Base PTC' # Name of column to be taken for base price for modelling\n\n# Time Data Column Names\nweekVariable = 'Time.[Week]' # Name of week granularity column\ndayVariable = 'Time.[Day]' # Name of day granularity column\nplanningMonthVariable = 'Time.[Planning Month]' # Name of planning month column\nweekNameVariable = 'Time.[Week Name]' # Name of week column\n\n# Promo Data Column Names\npromoMechanicColumnName = 'RGM Initiative Week Promo Mechanic' # Name of column from trade promo calendar data which we should use for promo mechanic.\npromoDisplayColumnName = 'RGM Initiative Week Display Type' # Name of column from trade promo calendar data which we should use for promo display.\nsellOutStartDateColumnName = 'RGM Initiative Week Sellout Start Date' # Name of column for sell out start date.\nsellOutEndDateColumnName = 'RGM Initiative Week Sellout End Date' # Name of column for sell out end date.\n\n# Defined column names for Audit Table:\nseriesVariable = 'Sequence.[RGM Series]'\nmodelRunTypeVariableName = 'RGM Model Run Type History' \npluginVariableName = 'RGM Plugin Name History'\nmachineLearningAlgorithmVariableName = 'RGM ML Algorithm History'\ndataStartDateVariableName = 'RGM Model Run Data Start Date History'\ndataEndDateVariableName = 'RGM Model Run Data End Date History'\npluginExecutionTimeStampVariableName = 'RGM Model Run Timestamp History'\npluginExecutionTimeVariableName = 'RGM Model Runtime History'\n\nsequenceId = 'RGM Model Run Sequence ID1'\n\n# Market Intel Data or External Data Column Names\nmarketIntelCountryVariable = 'Geo.[Country]'\n\n# Name of columne that we are using for building model.\nnameOfColumnForWhichBuildingModel = 'Item.[PPG]'\n\n# It is either freshModelRun, refreshWithoutModelRun, refreshWithModelRun \nmodelRunType = inputConfigParameter1['RGM Model Run Type'][0]\n\n# Choose the name of model to be used for Modelling i.e. from \"linearRegression\", \"constrainedLinearRegression\" and \"lsqLinear\"\nmodelName = inputConfigParameter1['RGM ML Algorithms'][0]\n\n\n# In[7]:\n\n\n# UserStory:212123:Due to regraning of sales domain, locationCountryVariable is same as countryVariable,i.e Region.[Region L3] will now contain the country info, to avoid the code change we are create the duplicate column\n# in the dataframe where we are using locationCountryVariable.\nsellOutData[locationCountryVariable]=sellOutData[countryVariable]\ntradePromoCalendarData[locationCountryVariable]=tradePromoCalendarData[countryVariable]\n\n\n# In[8]:\n\n\n# This is used for the configuration of external data to be used for modeling for individual countries.\n# e.g: variableName = {countryName: [listOfExternalDataFrames]}\n\n# Pass all the daily level external factors data frames in the form of list in the below shown dictionary.\ndictOfDailyLevelExternalData = {'USA':[], 'UK':[]}\n\n# Pass all the weekly level external factors data frames in the form of list in the below shown dictionary.\ndictOfWeeklyLevelExternalData = {'USA':[], 'UK':[]}\n\n# Pass all the monthly level external factors data frames in the form of list in the below shown dictionary.\ndictOfMonthlyLevelExternalData = {'USA':[], 'UK':[]}\n\n\n# In[9]:\n\n\n# This is like a configuration where user can input or change predefined values as per there requirement.\n\n# Seasonality Index should be calculate at 'Week' or 'Month' level depending on data granularity.\nseasonalityTimeGranularity = inputConfigParameter1['RGM Seasonality Time Bucket'][0]\n\n# Years for which we need to build the model.\ntry:\n    listOfModellingYear = [int(i.replace(' ','')) for i in inputConfigParameter1['RGM Historical Years'][0].split(',') ]\nexcept:\n    listOfModellingYear = [int(inputConfigParameter1['RGM Historical Years'])]\n\n# Year that we need to consider for seasonality calculation.\ntry:\n    listOfSeasonalityYear = [int(i.replace(' ','')) for i in inputConfigParameter1['RGM Seasonality Years'][0].split(',') ]\nexcept:\n    listOfSeasonalityYear = [int(inputConfigParameter1['RGM Seasonality Years'])]\n\n# Which promo method should be used? Don't keep both the method as 'Yes' at a time.\npromoMethod1 = 'Yes'  # It will create individual column for Display & Mechanics. \npromoMethod2 = 'No'   # It will create single column for Display & Mechanic by taking their cross product.\n\n# Which method should be used to merge sellOut and tradePromoCalendar data. \"method1\" or \"method2\"\npreparePromoDataMethod = \"method1\"\n\n\n# In[10]:\n\n\n# Input Variables Config:\n\n# List Of input variables for model.\nlistOfPromoMechanicsForModel = list(tradePromoCalendarData[promoMechanicColumnName].unique())\nlistOfPromoDisplayForModel = list(tradePromoCalendarData[promoDisplayColumnName].unique())\nlistOfInputVariablesForModel = ['Base Price', 'Discount Index', 'Weighted Distribution', 'Pantry Loading', 'Trendline', 'Seasonality Index']\nlistOfInputVariablesForModel.extend(listOfPromoMechanicsForModel)\nlistOfInputVariablesForModel.extend(listOfPromoDisplayForModel)\n\n# List Of input variables for model for which we need to take the log.\nlistOfInputVariablesWithLogForModel = ['Base Price', 'Discount Index', 'Weighted Distribution', 'Pantry Loading', 'Trendline', 'Seasonality Index']\n\n\n# In[11]:\n\n\nlogger.debug(\"Config setup is done successfully.\")\n\n\n# In[12]:\n\n\n# Create Date, Year, Month & Week columns.\ntimeDataOriginal = timeData.copy()\ntimeDataOriginal = timeDataOriginal[[weekVariable, weekNameVariable]].drop_duplicates().reset_index(drop=True)\ntimeDataOriginal[\"Year\"] = pd.DatetimeIndex(timeDataOriginal[weekVariable]).year\ntimeDataOriginal[\"Month\"] = pd.DatetimeIndex(timeDataOriginal[weekVariable]).month\ntimeDataOriginal['Week'] = timeDataOriginal[weekNameVariable].apply(lambda x: int(x[1:]))\ntimeDataOriginal = timeDataOriginal[[weekVariable, 'Year', 'Month', 'Week']]\n\ntimeData = timeData[[dayVariable, weekNameVariable]]\ntimeData['Week'] = timeData[weekNameVariable].apply(lambda x: int(x[1:]))\ntimeData = timeData[[dayVariable, 'Week']].rename(columns={dayVariable:weekVariable})\n\nsellOutData[\"Date\"] = pd.DatetimeIndex(sellOutData[weekVariable])\nsellOutData[\"Year\"] = pd.DatetimeIndex(sellOutData[weekVariable]).year\nsellOutData[\"Month\"] = pd.DatetimeIndex(sellOutData[weekVariable]).month\nsellOutData = sellOutData.merge(timeData, how='left', on=weekVariable)\n\n\n# In[13]:\n\n\n# timeData.to_csv('timeData.csv')\n# timeDataOriginal.to_csv('timeDataOriginal.csv')\n\n\n# In[14]:\n\n\n# Create a granularity variable at which we want to aggregate the data for the modelling.\nsellOutDataGranularityGroupBy = [versionVariable, countryVariable, regionVariable, locationCountryVariable, locationVariable, \n                                 channelVariable, customerGroupVariable, rgm1Variable, rgm2Variable, \n                                 rgm5Variable, rgm6Variable, rgm7Variable, ppgVariable, 'Year', 'Month', 'Week']\n\n\n# In[15]:\n\n\n# Filter out those rows which don't have any significant Volume and Value, sometimes you might get null.\nsellOutData = sellOutData[(sellOutData[valueSalesVariable]!=0) | (sellOutData[dependentVariable]!=0)].reset_index(drop=True)\nsellOutData = sellOutData[(sellOutData[valueSalesVariable].notna())| (sellOutData[dependentVariable].notna())].reset_index(drop=True)\nlogger.debug(\"For sellOutData, shape is \" + str(sellOutData.shape))\n#\n# Filter out those rows which don't have any start or end date Value, sometimes you might get null.\ntradePromoCalendarData = tradePromoCalendarData[(tradePromoCalendarData['RGM Initiative Week Sellout Start Date'].notna()) | (tradePromoCalendarData['RGM Initiative Week Sellout End Date'].notna())].reset_index(drop=True)\nlogger.debug(\"For tradePromoCalendarData, shape is \" + str(tradePromoCalendarData.shape))\n#\n# Filter out the sell Out data for the year we are modelling for.\nsellOutData = sellOutData[sellOutData['Year'].isin(listOfModellingYear)]\n\n\n# In[16]:\n\n\n# This method is used to fill missing dates in sell out data for each PPG on the basis of their Start and End Dates.\ndef fill_missing_rows_between_start_and_end_date_of_sellOut_data(dataFrameToBeFilled):\n    timeDummyData = pd.DataFrame(dataFrameToBeFilled[weekVariable].unique(), columns=[weekVariable]).merge(timeData, how='left', on=weekVariable)\n    timeDummyData['Year'] = pd.DatetimeIndex(timeDummyData[weekVariable]).year\n    timeDummyData['Month'] = pd.DatetimeIndex(timeDummyData[weekVariable]).month\n    timeDummyData['YMW'] = timeDummyData['Year'].astype(str) + timeDummyData['Month'].astype(str) + timeDummyData['Week'].astype(str)\n    timeDummyData.drop(columns=[weekVariable], inplace=True)\n    timeDummyData = timeDummyData[['Year', 'Month', 'Week', 'YMW']]\n    dataFrameToBeFilled['Granularity'] = dataFrameToBeFilled[versionVariable].astype(str) + dataFrameToBeFilled[countryVariable].astype(str) + dataFrameToBeFilled[regionVariable].astype(str) + dataFrameToBeFilled[locationVariable].astype(str) + dataFrameToBeFilled[locationCountryVariable].astype(str) + dataFrameToBeFilled[channelVariable].astype(str) + dataFrameToBeFilled[customerGroupVariable].astype(str) + dataFrameToBeFilled[rgm1Variable].astype(str) + dataFrameToBeFilled[rgm2Variable].astype(str) + dataFrameToBeFilled[rgm5Variable].astype(str) + dataFrameToBeFilled[rgm6Variable].astype(str) +  dataFrameToBeFilled[rgm7Variable].astype(str) + dataFrameToBeFilled[ppgVariable].astype(str)\n    dataFrameToBeFilled['YMW'] = dataFrameToBeFilled['Year'].astype(str) + dataFrameToBeFilled['Month'].astype(str) + dataFrameToBeFilled['Week'].astype(str)\n    fillMissingDate = pd.DataFrame()\n    for i in dataFrameToBeFilled[\"Granularity\"].unique():\n        timeDummy = timeDummyData.copy()\n        dataDummy = dataFrameToBeFilled[dataFrameToBeFilled[\"Granularity\"] == i].reset_index(drop=True)\n        startIndex = list(timeDummy['YMW']).index(dataDummy['YMW'][0])\n        endIndex = list(timeDummy['YMW']).index(dataDummy['YMW'][len(dataDummy)-1])\n        timeDummy = timeDummy[startIndex:endIndex+1].reset_index(drop=True)\n        dataDummy = timeDummy.merge(dataDummy, how='left', on=['Year', 'Month', 'Week', 'YMW']).reset_index(drop=True).fillna(0)\n        dataDummy.drop(columns=['YMW', 'Granularity'], inplace=True)\n        dataDummy[versionVariable] = dataDummy[versionVariable][0]\n        dataDummy[countryVariable] = dataDummy[countryVariable][0]\n        dataDummy[regionVariable] = dataDummy[regionVariable][0]\n        dataDummy[locationVariable] = dataDummy[locationVariable][0]\n        dataDummy[locationCountryVariable] = dataDummy[locationCountryVariable][0]\n        dataDummy[channelVariable] = dataDummy[channelVariable][0]\n        dataDummy[customerGroupVariable] = dataDummy[customerGroupVariable][0]\n        dataDummy[rgm1Variable] = dataDummy[rgm1Variable][0]\n        dataDummy[rgm2Variable] = dataDummy[rgm2Variable][0]\n        dataDummy[rgm5Variable] = dataDummy[rgm5Variable][0]\n        dataDummy[rgm6Variable] = dataDummy[rgm6Variable][0]\n        dataDummy[rgm7Variable] = dataDummy[rgm7Variable][0]\n        dataDummy[ppgVariable] = dataDummy[ppgVariable][0]\n        fillMissingDate = pd.concat([fillMissingDate, dataDummy], axis = 0).reset_index(drop=True)\n    dataFrameToBeFilled = fillMissingDate.copy()\n    return dataFrameToBeFilled\n\n\n# In[17]:\n\n\n# Calculating the selling price through Value and Volume columns using the formula mentioned below in the code.\ndef selling_price(sellOutData):\n    sellOutData[\"Selling Price\"] = sellOutData[valueSalesVariable]/sellOutData[dependentVariable]\n    return sellOutData\n\n\n# In[18]:\n\n\n# Renaming basePriceVariable to \"Base Price\".\ndef base_price(sellOutData):\n    sellOutData = selling_price(sellOutData)\n    sellOutData['Granularity'] = sellOutData[versionVariable].astype(str) + sellOutData[countryVariable].astype(str) + sellOutData[regionVariable].astype(str) + sellOutData[locationVariable].astype(str) + sellOutData[locationCountryVariable].astype(str) + sellOutData[channelVariable].astype(str) + sellOutData[customerGroupVariable].astype(str) + sellOutData[rgm1Variable].astype(str) + sellOutData[rgm2Variable].astype(str) + sellOutData[rgm5Variable].astype(str) + sellOutData[rgm6Variable].astype(str) + sellOutData[rgm7Variable].astype(str) + sellOutData[ppgVariable].astype(str)\n    salesData = sellOutData[sellOutDataGranularityGroupBy + [valueSalesVariable, dependentVariable, 'Granularity' ,'Selling Price', basePriceVariable]]\n    salesData = salesData.rename(columns={basePriceVariable:'Base Price'})\n    return salesData\n\n\n# In[19]:\n\n\n# This method is used to forward fill all the missing base price value.\ndef base_price_missing_values_treatment(salesData):\n    salesData['Base Price'] = np.where(salesData['Base Price']==0, np.nan, salesData['Base Price'])\n    salesData['Base Price'] = salesData['Base Price'].ffill(axis = 0)\n    return salesData\n\n\n# In[20]:\n\n\n# Calculating %discount and discount index using the formula mentioned below in the code.\ndef discount_index(sellOutData):\n    salesData = base_price(sellOutData)\n    salesDataInputConfigParameter2CommonColumns = [i for i in inputConfigParameter2.columns if i in salesData]\n    salesData = salesData.merge(inputConfigParameter2[salesDataInputConfigParameter2CommonColumns+['RGM Discount Pct Threshold']], how='left', on=salesDataInputConfigParameter2CommonColumns)\n    salesData[\"Discount\"] = ((salesData[\"Base Price\"] - salesData[\"Selling Price\"]) / salesData[\"Base Price\"])\n    salesData[\"Discount\"] = np.where(salesData[\"Discount\"]>=salesData['RGM Discount Pct Threshold'], salesData[\"Discount\"], 0)\n    salesData[\"Discount Index\"] = ((salesData[\"Selling Price\"]- salesData[\"Base Price\"]) / salesData[\"Base Price\"]) + 1\n    salesData[\"Discount Index\"] = np.where((salesData[\"Discount Index\"] != 1) & ((1 - salesData[\"Discount Index\"]) <= salesData['RGM Discount Pct Threshold']), 1, salesData[\"Discount Index\"])\n    salesData[\"Discount Index\"] = salesData[\"Discount Index\"].fillna(1)\n    salesData = salesData.drop(columns='RGM Discount Pct Threshold')\n    salesData = base_price_missing_values_treatment(salesData)\n    salesData = salesData.reset_index(drop=True)\n    return salesData\n\n\n# In[22]:\n\n\n# Renaming acvVariable to \"Weighted Distribution\".\ndef distribution(sellOutData):\n    data = sellOutData.copy()\n    data = data[sellOutDataGranularityGroupBy + [acvVariable]]\n    data = data.rename(columns={acvVariable:'Weighted Distribution'})\n    distributionData = data.copy()\n    return distributionData\n\n\n# In[23]:\n\n\n# Retailers or store will increase their shelf capacity whenever there is a high Discount Index running on a products.\n# We are setting pantry loading flag as 1 when Discount Index > 0 else it is 1 + decay rate * previous term.\ndef pantry_loading(salesData):\n    salesDataInputConfigParameter2CommonColumns = [i for i in inputConfigParameter2.columns if i in salesData]\n    salesData = salesData.merge(inputConfigParameter2[salesDataInputConfigParameter2CommonColumns+['RGM Pantry Decay Rate']], how='left', on=salesDataInputConfigParameter2CommonColumns)\n    pantryDataMain = pd.DataFrame()\n    for i in salesData[\"Granularity\"].unique():\n        pantryData = salesData[salesData[\"Granularity\"] == i]\n        pantryData = pantryData.reset_index(drop=True)\n        pantryData[\"Pantry Loading\"] = np.nan\n        for j in range(0, len(pantryData)):\n            if j == 0:\n                pantryData[\"Pantry Loading\"][j] = np.where(pantryData[\"Discount Index\"][j]<0, 1, 1)\n            if j>0:\n                pantryData[\"Pantry Loading\"][j] = np.where(pantryData[\"Discount Index\"][j]==1, np.max([(2-pantryData[\"Discount Index\"][j-1]), (pantryData[\"Pantry Loading\"][j-1] ** pantryData['RGM Pantry Decay Rate'][j])]), 1)\n        pantryData = pantryData.drop(columns='RGM Pantry Decay Rate')\n        pantryDataMain = pd.concat([pantryDataMain, pantryData]).reset_index(drop=True)\n    salesData = pantryDataMain.copy()\n    return salesData\n\n\n# In[24]:\n\n\n# P2 = MAX(P1,IF(LEFT(B2,1)<>LEFT(B1,1),1,0)) where 'P' is price threshold while 'B' is baseprice.\ndef price_threshold(salesData):\n    data = pd.DataFrame()\n    for i in salesData[\"Granularity\"].unique():\n        data_1 = salesData[salesData[\"Granularity\"] == i].reset_index(drop=True)\n        data_1[\"Price Threshold\"] = 0\n        for j in range(1, len(data_1)):\n            if int(str(data_1[\"Base Price\"][j]).split('.')[0]) != int(str(data_1[\"Base Price\"][j-1]).split('.')[0]):\n                data_1[\"Price Threshold\"][j] = max(1, data_1[\"Price Threshold\"][j-1])\n            else:\n                data_1[\"Price Threshold\"][j] = max(0, data_1[\"Price Threshold\"][j-1])\n        data = pd.concat([data, data_1], axis = 0).reset_index(drop=True)\n    salesData = data.copy()\n    return salesData\n\n\n# In[25]:\n\n\n# Merging trade promo calendar data with sellout data on the basis of sellout week fall under the sellout start and end date.\ndef merge_selloutdata_with_tradepromocalendardata_method1(sellOutData, tradePromoCalendarData, timeData):\n    tradePromoCalendarData[\"Year\"] = pd.DatetimeIndex(tradePromoCalendarData[weekVariable]).year\n    tradePromoCalendarData[\"Month\"] = pd.DatetimeIndex(tradePromoCalendarData[weekVariable]).month\n    tradePromoCalendarData = tradePromoCalendarData.merge(timeData, how='left', on=weekVariable)\n    tradePromoCalendarData['Display-Mechanic'] = tradePromoCalendarData[promoDisplayColumnName] + '-' + tradePromoCalendarData[promoMechanicColumnName]\n    sellOutTradePromoCommonColumns = [i for i in sellOutData.columns if i in tradePromoCalendarData.columns] + ['Year', 'Month', 'Week']\n    sellOutAndTradePromoData = sellOutData.merge(tradePromoCalendarData, how='left', on=sellOutTradePromoCommonColumns).fillna(0).reset_index(drop=True)\n    return sellOutAndTradePromoData\n\n\n# In[26]:\n\n\n# Merging trade promo calendar data with sellout data on the basis of sellout week fall under the sellout start and end date.\ndef merge_selloutdata_with_tradepromocalendardata_method2(sellOutData, tradePromoCalendarData, timeData):\n    timeData1 = timeData.copy()\n    timeData1[weekVariable] = pd.DatetimeIndex(timeData1[weekVariable])\n    tradePromoCalendarData = tradePromoCalendarData[(tradePromoCalendarData[promoMechanicColumnName].notnull()) | (tradePromoCalendarData[promoDisplayColumnName].notnull())].reset_index(drop=True)\n    tradePromoCalendarDataColumnsOriginalList = list(tradePromoCalendarData.columns)\n    tradePromoCalendarDataAfterColumnsDropList = tradePromoCalendarDataColumnsOriginalList.copy()\n    tradePromoCalendarDataAfterColumnsDropList.remove(sellOutStartDateColumnName)\n    tradePromoCalendarDataAfterColumnsDropList.remove(sellOutEndDateColumnName)\n    timeData1.rename(columns={weekVariable:sellOutStartDateColumnName}, inplace=True)\n    tradePromoCalendarData[sellOutStartDateColumnName] = pd.DatetimeIndex(tradePromoCalendarData[sellOutStartDateColumnName])\n    tradePromoCalendarData['SO Start Year'] = pd.DatetimeIndex(tradePromoCalendarData[sellOutStartDateColumnName]).year\n    tradePromoCalendarData['SO Start Month'] = pd.DatetimeIndex(tradePromoCalendarData[sellOutStartDateColumnName]).month\n    tradePromoCalendarData = tradePromoCalendarData.merge(timeData1, how='left', on=sellOutStartDateColumnName)\n    tradePromoCalendarData.rename(columns={'Week':'SO Start Week'}, inplace=True)\n    timeData1.rename(columns={sellOutStartDateColumnName:sellOutEndDateColumnName}, inplace=True)\n    tradePromoCalendarData[sellOutEndDateColumnName] = pd.DatetimeIndex(tradePromoCalendarData[sellOutEndDateColumnName])\n    tradePromoCalendarData['SO End Year'] = pd.DatetimeIndex(tradePromoCalendarData[sellOutEndDateColumnName]).year\n    tradePromoCalendarData['SO End Month'] = pd.DatetimeIndex(tradePromoCalendarData[sellOutEndDateColumnName]).month\n    tradePromoCalendarData = tradePromoCalendarData.merge(timeData1, how='left', on=sellOutEndDateColumnName)\n    tradePromoCalendarData.rename(columns={'Week':'SO End Week'}, inplace=True)\n    tradePromoCalendarData['SO Year Difference'] = tradePromoCalendarData['SO End Year'] - tradePromoCalendarData['SO Start Year']\n    tradePromoCalendarData['SO Month Difference'] = tradePromoCalendarData['SO End Month'] - tradePromoCalendarData['SO Start Month']\n    tradePromoCalendarData['SO Week Difference'] = tradePromoCalendarData['SO End Week'] - tradePromoCalendarData['SO Start Week']\n    maxWeek = tradePromoCalendarData.groupby(by='SO Start Year')['SO Start Week'].max().reset_index().rename(columns={'SO Start Week':'Maximum Week Number'})\n    tradePromoCalendarData = tradePromoCalendarData.merge(maxWeek, how='left', on='SO Start Year').reset_index(drop=True)\n    tradePromoCalendarData['SO Month Difference'] = np.where(tradePromoCalendarData['SO Month Difference'] < 0, 12 + tradePromoCalendarData['SO End Month'] - tradePromoCalendarData['SO Start Month'], tradePromoCalendarData['SO Month Difference'])\n    tradePromoCalendarData['SO Week Difference'] = np.where(tradePromoCalendarData['SO Week Difference'] < 0, tradePromoCalendarData['Maximum Week Number'] + tradePromoCalendarData['SO End Week'] - tradePromoCalendarData['SO Start Week'], tradePromoCalendarData['SO Week Difference'])\n    tradePromoCalendarDataDataFrame1 = tradePromoCalendarData[(tradePromoCalendarData['SO Week Difference']==0) | (tradePromoCalendarData['SO Week Difference']==1)].reset_index(drop=True)\n    tradePromoCalendarDataDataFrame2 = tradePromoCalendarData[tradePromoCalendarData['SO Week Difference']>1].reset_index(drop=True)\n    tradePromoCalendarDataDataFrame3 = pd.DataFrame()\n    for i in range(0, len(tradePromoCalendarDataDataFrame2)):\n        data = tradePromoCalendarDataDataFrame2[i:i+1].reset_index(drop=True)\n        sellOutStartDateList = pd.date_range(start = data[sellOutStartDateColumnName][0], periods=data['SO Week Difference'][0], freq='W') \n        sellOutEndDate = data[sellOutEndDateColumnName][0]\n        dateListDataFrame = pd.DataFrame({ sellOutEndDateColumnName:sellOutEndDate, 'Sell Out Start Date':sellOutStartDateList})\n        dataDummy = dateListDataFrame.merge(data, how='left', on=sellOutEndDateColumnName)\n        dataDummy[sellOutStartDateColumnName] = pd.DatetimeIndex(dataDummy['Sell Out Start Date'])\n        dataDummy.drop(columns='Sell Out Start Date', inplace=True)\n        dataDummy = dataDummy[data.columns]\n        tradePromoCalendarDataDataFrame3 = pd.concat([tradePromoCalendarDataDataFrame3, dataDummy]).reset_index(drop=True)\n    tradePromoCalendarData = pd.concat([tradePromoCalendarDataDataFrame1, tradePromoCalendarDataDataFrame3]).reset_index(drop=True)\n    tradePromoCalendarDataMelt = pd.melt(tradePromoCalendarData, id_vars=tradePromoCalendarDataAfterColumnsDropList, value_vars=[sellOutStartDateColumnName, sellOutEndDateColumnName], var_name='Trade Promo Date Type', value_name='Trade Promo Date')\n    tradePromoCalendarDataMelt['Display-Mechanic'] = tradePromoCalendarDataMelt[promoDisplayColumnName] + '-' + tradePromoCalendarDataMelt[promoMechanicColumnName]\n    tradePromoCalendarDataMelt['Year'] = pd.DatetimeIndex(tradePromoCalendarDataMelt['Trade Promo Date']).year\n    tradePromoCalendarDataMelt['Month'] = pd.DatetimeIndex(tradePromoCalendarDataMelt['Trade Promo Date']).month\n    timeData1.rename(columns={sellOutEndDateColumnName:'Trade Promo Date'}, inplace=True)\n    tradePromoCalendarDataMelt = tradePromoCalendarDataMelt.merge(timeData1, how='left', on='Trade Promo Date')\n    tradePromoCalendarDataMelt.drop_duplicates(inplace=True)\n    sellOutTradePromoCommonColumns = [i for i in sellOutData.columns if i in tradePromoCalendarData.columns] + ['Year', 'Month', 'Week']\n    sellOutAndTradePromoData = sellOutData.merge(tradePromoCalendarDataMelt, how='left', on=sellOutTradePromoCommonColumns).fillna(0).reset_index(drop=True)\n    return sellOutAndTradePromoData\n\n\n# In[27]:\n\n\ndef prepare_promo_data(sellOutData, tradePromoCalendarData, timeData):\n    if preparePromoDataMethod == \"method1\":\n        sellOutAndTradePromoData = merge_selloutdata_with_tradepromocalendardata_method1(sellOutData, tradePromoCalendarData, timeData)\n    elif preparePromoDataMethod == \"method2\":\n        sellOutAndTradePromoData = merge_selloutdata_with_tradepromocalendardata_method2(sellOutData, tradePromoCalendarData, timeData)\n    else:\n        logger.debug(\"preparePromoDataMethod variable is missing.\")\n    sellOutAndTradePromoData = sellOutAndTradePromoData[(sellOutAndTradePromoData[promoMechanicColumnName]!=0) | (sellOutAndTradePromoData[promoDisplayColumnName]!=0)].reset_index(drop=True)\n    sellOutAndTradePromoData['Intermediate Promo Flag'] = 1\n    if promoMethod1 == 'Yes':\n        promoMechanicData = sellOutAndTradePromoData.pivot_table(index=sellOutDataGranularityGroupBy, columns=promoMechanicColumnName, values='Intermediate Promo Flag', aggfunc=np.mean).reset_index()\n        promoMechanicData = promoMechanicData.rename_axis(None, axis=1)\n        promoDisplayData = sellOutAndTradePromoData.pivot_table(index=sellOutDataGranularityGroupBy, columns=promoDisplayColumnName, values='Intermediate Promo Flag', aggfunc=np.mean).reset_index()\n        promoDisplayData = promoDisplayData.rename_axis(None, axis=1)\n        promoMechanicAndDisplayData = promoDisplayData.merge(promoMechanicData, how='left', on=sellOutDataGranularityGroupBy).fillna(0)\n    if promoMethod2 == 'Yes':\n        promoMechanicAndDisplayData = sellOutAndTradePromoData.pivot_table(index=sellOutDataGranularityGroupBy, columns='Display-Mechanic', values='Intermediate Promo Flag', aggfunc=np.mean).reset_index()\n        promoMechanicAndDisplayData = promoMechanicAndDisplayData.rename_axis(None, axis=1)\n    return promoMechanicAndDisplayData\n\n\n# In[28]:\n\n\n# Trendline is a 45 degree trend line used to capture the sales trend over the period of time.\ndef trend_line(salesData):\n    trendDataMain = pd.DataFrame()\n    for i in salesData[\"Granularity\"].unique():\n        trendData = salesData[salesData[\"Granularity\"] == i]\n        trendData = trendData.reset_index(drop=True)\n        trendData[\"Trendline\"] = np.nan\n        for j in range(0, len(trendData)):\n            trendData[\"Trendline\"][j] = j+1.5\n        trendDataMain = pd.concat([trendDataMain, trendData]).reset_index(drop=True)\n    salesData = trendDataMain.copy()\n    return salesData\n\n\n# In[29]:\n\n\n# Creating a holiday flag to capture effect of holiday on sales whenever a one or more holidays are falling in a week.\ndef holiday(holidayData, timeData):\n    timeData1 = timeData.copy()\n    holidayData[\"Date\"] = pd.DatetimeIndex(holidayData[weekVariable])\n    holidayData[\"Year\"] = pd.DatetimeIndex(holidayData[weekVariable]).year\n    holidayData[\"Month\"] = pd.DatetimeIndex(holidayData[weekVariable]).month\n    holidayData = holidayData.merge(timeData1, how='left', on=weekVariable)\n    holidayData[\"Holiday Flag\"] = 1\n    holidayData = holidayData[[versionVariable, locationCountryVariable, 'Year', 'Month', 'Week', 'Holiday Flag']].drop_duplicates()\n    return holidayData\n\n\n# In[30]:\n\n\n# Creating a flag to capture effect of power date on sales whenever a one or more power date are falling in a week.\ndef power_date(powerDateData, timeData):\n    timeData1 = timeData.copy()\n    powerDateData[\"Date\"] = pd.DatetimeIndex(powerDateData[dayVariable])\n    powerDateData[\"Year\"] = pd.DatetimeIndex(powerDateData[dayVariable]).year\n    powerDateData[\"Month\"] = pd.DatetimeIndex(powerDateData[dayVariable]).month\n    powerDateData = powerDateData.merge(timeData1, how='left', on=weekVariable)\n    powerDateData[\"Power Date Flag\"] = 1\n    powerDateData = powerDateData[[versionVariable, locationCountryVariable, rgm5Variable, 'Year', 'Month', 'Week', 'Power Date Flag']].drop_duplicates()\n    return powerDateData\n\n\n# In[31]:\n\n\n# Creating Year and Month columns using 'Time.[Planning Month]' column. \ndef planning_month_year(data, centuryFirstTwoInitial = '20'):\n    data['Year'] = data[planningMonthVariable].apply(lambda x: int(centuryFirstTwoInitial + x.split('-')[1]))\n    data['Month'] = data[planningMonthVariable].apply(lambda x: int(x.split('-')[0][1:]) if '0' not in x.split('-')[0][1:2] else int(x.split('-')[0][2:]))\n    return data\n\n\n# In[32]:\n\n\ndef day_to_week(data, timeData):\n    timeData1 = timeData.copy()\n    data['Date'] = pd.DatetimeIndex(data[dayVariable])\n    data['Year'] = pd.DatetimeIndex(data[dayVariable]).year\n    data['Month'] = pd.DatetimeIndex(data[dayVariable]).month\n    timeData1.rename(columns={weekVariable:dayVariable}, inplace=True)\n    data = data.merge(timeData1, how='left', on=dayVariable)\n    return data\n\n\n# In[33]:\n\n\ndef daily_level_external_factors(countryName, listOfDailyLevelExternalData, timeData):\n    externalDict = {}\n    count = 1\n    for i in listOfDailyLevelExternalData:\n        name = str('externalDataFrame' + countryName + str(count))\n        externalDataFrame = pd.DataFrame()\n        externalDataFrame = day_to_week(i, timeData)\n        externalDataFrame = externalDataFrame.drop(columns=['Date', dayVariable])\n        externalDataFrame = externalDataFrame.groupby(['Year', 'Month', 'Week', versionVariable, marketIntelCountryVariable]).mean().reset_index()\n        externalDataFrame.rename(columns={marketIntelCountryVariable:locationCountryVariable}, inplace=True)\n        externalDataFrame = externalDataFrame[externalDataFrame[locationCountryVariable]==countryName]\n        externalDict[name] = externalDataFrame\n        count = count + 1\n    return externalDict\n\n\n# In[34]:\n\n\ndef weekly_level_external_factors(countryName, listOfWeeklyLevelExternalData, timeData):\n    timeData1 = timeData.copy()\n    externalDict = {}\n    count = 1\n    for i in listOfWeeklyLevelExternalData:\n        name = str('externalDataFrame' + countryName + str(count))\n        externalDataFrame = i.copy()\n        externalDataFrame['Date'] = pd.DatetimeIndex(externalDataFrame[weekVariable])\n        externalDataFrame['Year'] = pd.DatetimeIndex(externalDataFrame[weekVariable]).year\n        externalDataFrame['Month'] = pd.DatetimeIndex(externalDataFrame[weekVariable]).month\n        externalDataFrame = externalDataFrame.merge(timeData1, how='left', on=weekVariable)\n        externalDataFrame = externalDataFrame.drop(columns=['Date', weekVariable])\n        externalDataFrame.rename(columns={marketIntelCountryVariable:locationCountryVariable}, inplace=True)\n        externalDataFrame = externalDataFrame[externalDataFrame[locationCountryVariable]==countryName]\n        externalDict[name] = externalDataFrame\n        count = count + 1\n    return externalDict\n\n\n# In[35]:\n\n\ndef monthly_level_macroeconomic(countryName, listOfMonthlyLevelMacroeconomicData):\n    macroeconomicDict = {}\n    count = 1\n    for i in listOfMonthlyLevelMacroeconomicData:\n        name = str('macroeconomicDataFrame' + countryName + str(count))\n        macroeconomicDataFrame = pd.DataFrame()\n        macroeconomicDataFrame = planning_month_year(i)\n        macroeconomicDataFrame = macroeconomicDataFrame.drop(columns=[planningMonthVariable])\n        macroeconomicDataFrame.rename(columns={marketIntelCountryVariable:locationCountryVariable}, inplace=True)\n        macroeconomicDataFrame = macroeconomicDataFrame[macroeconomicDataFrame[locationCountryVariable]==countryName]\n        macroeconomicDict[name] = macroeconomicDataFrame\n        count = count + 1\n    return macroeconomicDict\n\n\n# In[36]:\n\n\ndef loyalty(sellOutData, loyaltyData):\n    mergeGranularity = [i for i in loyaltyData.columns if i in sellOutData.columns]\n    sellOutAndLoyaltyData = sellOutData.merge(loyaltyData, how='left', on=mergeGranularity)\n    loyaltyDataStructured = sellOutAndLoyaltyData.groupby(by=sellOutDataGranularityGroupBy)[\"RGM Loyalty\"].mean().reset_index()\n    return loyaltyDataStructured\n\n\n# In[37]:\n\n\n# Here we are calculating seasonality index by taking ratio of weekly average of sales for all year to overall average of sales.\ndef seasonality_index(salesData, listOfSeasonalityYear):\n    data = salesData.groupby(by=['Year', seasonalityTimeGranularity, rgm5Variable, locationCountryVariable, channelVariable])[dependentVariable].sum().reset_index()\n    data = data[data['Year'].isin(listOfSeasonalityYear)].reset_index(drop=True)\n    data['Key'] = data[rgm5Variable] + data[locationCountryVariable] + data[channelVariable]\n    data = data.pivot_table(index=[seasonalityTimeGranularity, rgm5Variable, locationCountryVariable, channelVariable, 'Key'], columns='Year', values=dependentVariable, aggfunc=np.sum).reset_index().fillna(0)\n    data = data.rename_axis(None, axis=1)\n    seasonalityDataMain = pd.DataFrame()\n    for i in data[\"Key\"].unique():\n        seasonalityData = data[data[\"Key\"]==i].reset_index(drop=True)\n        seasonalityDataCount = seasonalityData.iloc[:,len(seasonalityData.columns)-2:len(seasonalityData.columns)]\n        seasonalityDataCount['Count'] = np.count_nonzero(seasonalityDataCount, axis=1)\n        seasonalityData['Count'] = seasonalityDataCount['Count']\n        if len(listOfSeasonalityYear) == 1:\n            seasonalityData[\"Weekly Average\"] = (seasonalityData[listOfSeasonalityYear[0]])/seasonalityData['Count']\n        elif len(listOfSeasonalityYear) == 2:\n            seasonalityData[\"Weekly Average\"] = (seasonalityData[listOfSeasonalityYear[0]] + seasonalityData[listOfSeasonalityYear[1]])/seasonalityData['Count']\n        elif len(listOfSeasonalityYear) == 3:\n            seasonalityData[\"Weekly Average\"] = (seasonalityData[listOfSeasonalityYear[0]] + seasonalityData[listOfSeasonalityYear[1]] + seasonalityData[listOfSeasonalityYear[2]])/seasonalityData['Count']\n        elif len(listOfSeasonalityYear) == 4:\n            seasonalityData[\"Weekly Average\"] = (seasonalityData[listOfSeasonalityYear[0]] + seasonalityData[listOfSeasonalityYear[1]] + seasonalityData[listOfSeasonalityYear[2]] + seasonalityData[listOfSeasonalityYear[3]])/seasonalityData['Count']\n        elif len(listOfSeasonalityYear) == 5:\n            seasonalityData[\"Weekly Average\"] = (seasonalityData[listOfSeasonalityYear[0]] + seasonalityData[listOfSeasonalityYear[1]] + seasonalityData[listOfSeasonalityYear[2]] + seasonalityData[listOfSeasonalityYear[3]] + seasonalityData[listOfSeasonalityYear[4]])/seasonalityData['Count']\n        elif len(listOfSeasonalityYear) == 6:\n            seasonalityData[\"Weekly Average\"] = (seasonalityData[listOfSeasonalityYear[0]] + seasonalityData[listOfSeasonalityYear[1]] + seasonalityData[listOfSeasonalityYear[2]] + seasonalityData[listOfSeasonalityYear[3]] + seasonalityData[listOfSeasonalityYear[4]] + seasonalityData[listOfSeasonalityYear[5]])/seasonalityData['Count']\n        else:\n            logger.debug('Number of seasonality years should be less than or equal to 6.')\n        seasonalityData[\"Overall Average\"] = np.average(seasonalityData[\"Weekly Average\"])\n        seasonalityData[\"Seasonality Index\"] = seasonalityData[\"Weekly Average\"]/seasonalityData[\"Overall Average\"]\n        seasonalityDataMain = pd.concat([seasonalityDataMain, seasonalityData]).reset_index(drop=True)\n        seasonalityDataMain.drop(columns=['Key', 'Weekly Average', 'Overall Average', 'Count']+listOfSeasonalityYear, inplace=True)\n    return seasonalityDataMain\n\n\n# In[38]:\n\n\ndef adstock(currentValue, previousValue, decayRate):\n    pass\n\n\n# In[39]:\n\n\n# Calculating correlation between each and every column.\ndef correlation_check(data):\n    data = data.drop(columns=['Year', 'Month', 'Week'])\n    correlation_data = data.corr(method ='pearson')\n    return correlation_data\n\n\n# In[40]:\n\n\n# Calculating vif between model input variables to avoid multi-collinearity.\ndef vif_check(data, listOfInputVariablesForModel):\n    X = data[listOfInputVariablesForModel]\n    vif_data = pd.DataFrame()\n    vif_data[\"Feature\"] = X.columns\n    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n    return vif_data\n\n\n# In[41]:\n\n\n# Merging output of an individual methods to prepare sales data.\ndef sales_data_prep():\n    salesData = discount_index(sellOutData)\n    distributionData = distribution(sellOutData)\n    salesData = salesData.merge(distributionData, how='inner', on=sellOutDataGranularityGroupBy).reset_index(drop = True)\n    salesData = pantry_loading(salesData)\n    return salesData\n\n\n# In[42]:\n\n\n# Running individual methods to create master data for modelling.\ndef master_data_prep():\n    salesData = sales_data_prep()\n    logger.debug(\"Successfully derived variables for weighted distribution, price, discount & pantry loading.\")\n    \n    salesData = price_threshold(salesData)\n    salesData = trend_line(salesData)\n    seasonalityIndex = seasonality_index(salesData, listOfSeasonalityYear)\n    salesData = salesData.merge(seasonalityIndex, how='left', on=[seasonalityTimeGranularity, rgm5Variable, locationCountryVariable, channelVariable])\n    logger.debug(\"Successfully derived variables for trendline & seasonality index.\")\n    \n    try:\n        promoMechanicAndDisplayData = prepare_promo_data(sellOutData, tradePromoCalendarData, timeData)\n        salesData = salesData.merge(promoMechanicAndDisplayData, how='left', on=sellOutDataGranularityGroupBy).reset_index(drop=True)\n    except:\n        logger.debug(\"Warning: Promo data is missing.\")\n        \n    try:\n        loyaltyDataStructured = loyalty(sellOutData, loyaltyData)\n        salesData = salesData.merge(loyaltyDataStructured, how='left', on=sellOutDataGranularityGroupBy).reset_index(drop=True)\n    except:\n        logger.debug(\"Warning: Loyalty data is missing.\")\n    \n    salesDataAndExternalData = salesData[~salesData[locationCountryVariable].isin(list(dictOfDailyLevelExternalData.keys()))]\n    for countryName, externalData in dictOfDailyLevelExternalData.items():\n        salesDataDummy = salesData[salesData[locationCountryVariable]==countryName]\n        externalDict1 = {}\n        countryName = countryName\n        listOfDailyLevelExternalData = externalData\n        if len(listOfDailyLevelExternalData)!=0:\n            externalDict0 = daily_level_external_factors(countryName, listOfDailyLevelExternalData, timeData)\n            externalDict1.update(externalDict0)\n            for i in (list(externalDict1.keys())):\n                externalDataFrame = externalDict1[i]\n                salesDataDummy = salesDataDummy.merge(externalDataFrame, how='left', on=['Year', 'Month', 'Week', versionVariable, locationCountryVariable])\n        else:\n            logger.debug(\"Warning: Daily level external data is missing for \" + countryName + \".\")\n        salesDataAndExternalData = pd.concat([salesDataAndExternalData, salesDataDummy], axis=0).reset_index(drop=True)\n    salesData = salesDataAndExternalData.copy()\n    \n    salesDataAndExternalData = salesData[~salesData[locationCountryVariable].isin(list(dictOfWeeklyLevelExternalData.keys()))]\n    for countryName, externalData in dictOfWeeklyLevelExternalData.items():\n        salesDataDummy = salesData[salesData[locationCountryVariable]==countryName]\n        externalDict1 = {}\n        countryName = countryName\n        listOfWeeklyLevelExternalData = externalData\n        if len(listOfWeeklyLevelExternalData)!=0:\n            externalDict0 = weekly_level_external_factors(countryName, listOfWeeklyLevelExternalData, timeData)\n            externalDict1.update(externalDict0)\n            for i in (list(externalDict1.keys())):\n                externalDataFrame = externalDict1[i]\n                salesDataDummy = salesDataDummy.merge(externalDataFrame, how='left', on=['Year', 'Month', 'Week', versionVariable, locationCountryVariable])\n        else:\n            logger.debug(\"Warning: Weekly level external data is missing for \" + countryName + \".\")\n        salesDataAndExternalData = pd.concat([salesDataAndExternalData, salesDataDummy], axis=0).reset_index(drop=True)\n    salesData = salesDataAndExternalData.copy()\n    \n    salesDataAndExternalData = salesData[~salesData[locationCountryVariable].isin(list(dictOfMonthlyLevelExternalData.keys()))]\n    for countryName, externalData in dictOfMonthlyLevelExternalData.items():\n        salesDataDummy = salesData[salesData[locationCountryVariable]==countryName]\n        macroeconomicDict1 = {}\n        countryName = countryName\n        listOfMonthlyLevelMacroeconomicData = externalData\n        if len(listOfMonthlyLevelMacroeconomicData)!=0:\n            macroeconomicDict0 = monthly_level_macroeconomic(countryName, listOfMonthlyLevelMacroeconomicData)\n            macroeconomicDict1.update(macroeconomicDict0)\n            for i in (list(macroeconomicDict1.keys())):\n                externalDataFrame = macroeconomicDict1[i]\n                salesDataDummy = salesDataDummy.merge(externalDataFrame, how='left', on=['Year', 'Month', versionVariable, locationCountryVariable])\n        else:\n            logger.debug(\"Warning: Monthly level external data is missing for \" + countryName + \".\")\n        salesDataAndExternalData = pd.concat([salesDataAndExternalData, salesDataDummy], axis=0).reset_index(drop=True)\n    salesData = salesDataAndExternalData.copy()\n    \n    salesData.drop(columns = \"Granularity\", inplace=True)\n    \n    try:\n        holidayData1 = holiday(holidayData, timeData)\n        salesData = salesData.merge(holidayData1, how='left', on=[versionVariable, locationCountryVariable, 'Year', 'Month', 'Week']).fillna(0)\n    except:\n        logger.debug(\"Warning: Holiday data is missing.\")\n    \n    try:\n        powerDateData1 = power_date(powerDateData, timeData)\n        salesData = salesData.merge(powerDateData1, how='left', on=[versionVariable, locationCountryVariable, rgm5Variable, 'Year', 'Month', 'Week']).fillna(0)\n    except:\n        logger.debug(\"Warning: Power date data is missing.\")\n    \n    masterData = salesData.copy()\n    masterData = masterData.reset_index(drop=True)\n    masterData.fillna(0, inplace=True)\n    return masterData\n\n\n# In[43]:\n\n\nsellOutData = fill_missing_rows_between_start_and_end_date_of_sellOut_data(sellOutData)\nmasterData = master_data_prep()\n\n\n# In[44]:\n\n\nlogger.debug(\"Successfully prepared the master data.\")\n\n\n# In[45]:\n\n\n# Converting master data from wide format to long format.\nmasterData = pd.melt(masterData, id_vars=sellOutDataGranularityGroupBy, value_vars=[i for i in masterData.columns if i not in sellOutDataGranularityGroupBy], var_name='Promo Simulation.[Promo Variables]', value_name='RGM Promo Simulation Script 1 Output Placeholder')\n\n\n# In[46]:\n\n\nmasterData = masterData.merge(timeDataOriginal, how='left', on=['Year', 'Month', 'Week']).drop(columns=['Year', 'Month', 'Week'])\n\n\n# In[47]:\n\n\nmasterDataAudit = masterData.copy()\n\n\n# In[48]:\n\n\nmasterData = masterData[[versionVariable, countryVariable, regionVariable, \n                         locationCountryVariable, locationVariable, \n                         channelVariable, customerGroupVariable, \n                         rgm1Variable, rgm2Variable, rgm5Variable, rgm6Variable, rgm7Variable, ppgVariable, weekVariable, \n                         'Promo Simulation.[Promo Variables]', 'RGM Promo Simulation Script 1 Output Placeholder']]\n\n\n# In[49]:\n\n\n# Preparing data for audit table.\n\n# Create an empty prepareCurrentAuditDataFrame with header names.\nauditColumns = [versionVariable, seriesVariable, dayVariable, modelRunTypeVariableName, pluginVariableName, machineLearningAlgorithmVariableName, dataStartDateVariableName, dataEndDateVariableName, pluginExecutionTimeStampVariableName, pluginExecutionTimeVariableName]\nprepareCurrentAuditDataFrame = pd.DataFrame(columns = auditColumns)\n\n# Get the last date for which we processed the sellout data from the audit table.\ntry:\n    if modelRunType in ['refreshWithoutModelRun', 'refreshWithModelRun']:\n        auditDataFrame1 = auditDataFrame[(auditDataFrame[pluginVariableName]=='Postprocessing')]\n        auditDataFrame2 = auditDataFrame1[(pd.to_datetime(auditDataFrame1[pluginExecutionTimeStampVariableName])==pd.to_datetime(auditDataFrame1[pluginExecutionTimeStampVariableName]).max())]\n        lastProcessedDate = pd.to_datetime(auditDataFrame2[dataEndDateVariableName]).max().strftime(\"%d-%b-%y\")\nexcept:\n    if auditDataFrame.empty:\n        modelRunType = 'freshModelRun'\n\nif modelRunType in ['freshModelRun']:\n    prepareCurrentAuditDataFrame[dataStartDateVariableName] = [pd.to_datetime(masterDataAudit[weekVariable]).min().strftime(\"%d-%b-%y\")]\n    prepareCurrentAuditDataFrame[dataEndDateVariableName] = [pd.to_datetime(masterDataAudit[weekVariable]).max().strftime(\"%d-%b-%y\")]\nelif modelRunType in ['refreshWithoutModelRun', 'refreshWithModelRun']:\n    try:\n        prepareCurrentAuditDataFrame[dataStartDateVariableName] = [pd.to_datetime(masterDataAudit[(pd.to_datetime(masterDataAudit[weekVariable]) > pd.to_datetime(lastProcessedDate))][weekVariable]).min().strftime(\"%d-%b-%y\")]\n    except:\n        prepareCurrentAuditDataFrame[dataStartDateVariableName] = [pd.to_datetime(masterDataAudit[(pd.to_datetime(masterDataAudit[weekVariable]) >= pd.to_datetime(lastProcessedDate))][weekVariable]).min().strftime(\"%d-%b-%y\")]\n    try:\n        prepareCurrentAuditDataFrame[dataEndDateVariableName] = [pd.to_datetime(masterDataAudit[(pd.to_datetime(masterDataAudit[weekVariable]) > pd.to_datetime(lastProcessedDate))][weekVariable]).max().strftime(\"%d-%b-%y\")]\n    except:\n        prepareCurrentAuditDataFrame[dataEndDateVariableName] = [pd.to_datetime(masterDataAudit[(pd.to_datetime(masterDataAudit[weekVariable]) >= pd.to_datetime(lastProcessedDate))][weekVariable]).max().strftime(\"%d-%b-%y\")]\n\n        \n# Get current timestamp and duration of plugin execution.\nnow = datetime.datetime.now()\ncurrTimestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\nendTime = time.time()\n\nprepareCurrentAuditDataFrame[versionVariable] = [masterDataAudit[versionVariable].unique()[0]]\nprepareCurrentAuditDataFrame[seriesVariable] = [int(tenantSequenceID[sequenceId].loc[0])]\nprepareCurrentAuditDataFrame[dayVariable] = [tenantCurrDay[dayVariable].loc[0]]\n\nprepareCurrentAuditDataFrame[pluginVariableName] = ['Preprocessing']\n# prepareCurrentAuditDataFrame[machineLearningAlgorithmVariableName] = [modelName] \nprepareCurrentAuditDataFrame[modelRunTypeVariableName] = [modelRunType]  # Either 'freshModelRun', 'refreshWithoutModelRun', 'refreshWithModelRun'\nprepareCurrentAuditDataFrame[pluginExecutionTimeStampVariableName] = [pd.to_datetime(currTimestamp)] # Get current timestamp\npluginExecutionTimeInMinutes = np.round((endTime-startTime)/60,2)\nprepareCurrentAuditDataFrame[pluginExecutionTimeVariableName] = [pluginExecutionTimeInMinutes]  # Get duration of plugin execution\n\nauditDataFrame = auditDataFrame.append(prepareCurrentAuditDataFrame, ignore_index=True)\nauditDataFrameCopy = auditDataFrame.copy()\n\n\n# In[50]:\n\n\n## Output for Tenant\n## Uncomment When Required\n\n#UserStory:212123 - channelVariable is one of the least granual under Channel dimension so added in all the output\nmasterData = masterData[[versionVariable,channelVariable, locationVariable, customerGroupVariable, ppgVariable, weekVariable,\n                         'Promo Simulation.[Promo Variables]', 'RGM Promo Simulation Script 1 Output Placeholder']]\n\n\n# In[51]:\n\n\nlogger.debug(\"For masterData, shape is \" + str(masterData.shape))\nlogger.debug(\"For auditDataFrameCopy, shape is \" + str(auditDataFrameCopy.shape))\nlogger.debug(\"Pre-processing code for promo model is ran successfully and output is saved.\")\n\n\n# In[52]:\n\n\n# masterData.to_csv('Master Data.csv', index=False)\n# auditDataFrameCopy.to_csv('auditTableDataOutputFinal.csv', index=False)\n\n\n"], "metadata": {}}], "metadata": {"kernelspec": {"display_name": "RGMC2DS", "language": "python", "name": "RGMC2DS"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.10"}, "notebook_dict": {"file_path": "loaded_notebooks/RGMPromoModelingPreprocessingPlugin1.ipynb", "ClassName": "o9.GraphCube.Plugins.Python.PythonScript", "InstanceName": "RGMPromoModelingPreprocessingPlugin1", "SliceKeys": [], "o9_selected_plugin_id": 1089282}}, "nbformat": 4, "nbformat_minor": 5}